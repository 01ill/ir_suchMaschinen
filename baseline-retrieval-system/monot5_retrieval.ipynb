{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einrichtung\n",
    "- Installieren der Packages\n",
    "- Initialisieren von Pyterrier\n",
    "- Laden der Datensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (0.0.143)\n",
      "Requirement already satisfied: ir-datasets in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (0.5.9)\n",
      "Requirement already satisfied: python-terrier in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from tira) (2.32.3)\n",
      "Requirement already satisfied: docker==7.*,>=7.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from tira) (7.1.0)\n",
      "Requirement already satisfied: numpy==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from tira) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from tira) (2.2.3)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from tira) (24.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from tira) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from docker==7.*,>=7.1.0->tira) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests==2.*,>=2.26->tira) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests==2.*,>=2.26->tira) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests==2.*,>=2.26->tira) (2024.8.30)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (4.12.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (5.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (6.0.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (10.5.0)\n",
      "Requirement already satisfied: wget in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: deprecated in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (1.2.15)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (1.14.1)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (0.3.6)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (3.1.4)\n",
      "Requirement already satisfied: statsmodels in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (0.14.4)\n",
      "Requirement already satisfied: dill in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (0.3.9)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (1.4.2)\n",
      "Requirement already satisfied: chest in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.6)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from deprecated->python-terrier) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from jinja2->python-terrier) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pandas->tira) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pandas->tira) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pandas->tira) (2024.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from statsmodels->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->tira) (1.17.0)\n",
      "Requirement already satisfied: pyterrier-caching in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: pyterrier_t5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: python-terrier>=0.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (0.13.0)\n",
      "Requirement already satisfied: pyterrier-alpha>=0.9.4 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (0.11.1)\n",
      "Requirement already satisfied: lz4 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (1.26.4)\n",
      "Requirement already satisfied: npids>=0.0.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (0.0.7)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (3.12.1)\n",
      "Requirement already satisfied: deprecated in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-caching) (1.2.15)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier_t5) (4.47.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier_t5) (2.5.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier_t5) (0.2.0)\n",
      "Requirement already satisfied: colaburl in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pyterrier-alpha>=0.9.4->pyterrier-caching) (0.1.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (2.2.3)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (10.5.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (2.32.3)\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (0.5.9)\n",
      "Requirement already satisfied: wget in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (3.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (1.14.1)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (0.3.6)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (0.5.6)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (3.1.4)\n",
      "Requirement already satisfied: statsmodels in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (0.14.4)\n",
      "Requirement already satisfied: dill in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (0.3.9)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (1.4.2)\n",
      "Requirement already satisfied: chest in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-terrier>=0.11.0->pyterrier-caching) (0.2.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (0.26.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from transformers>=4.0.0->pyterrier_t5) (0.4.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from deprecated->pyterrier-caching) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from torch->pyterrier_t5) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from torch->pyterrier_t5) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from torch->pyterrier_t5) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from torch->pyterrier_t5) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from sympy==1.13.1->torch->pyterrier_t5) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (4.12.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (5.3.0)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (2.6)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (0.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests->python-terrier>=0.11.0->pyterrier-caching) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests->python-terrier>=0.11.0->pyterrier-caching) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests->python-terrier>=0.11.0->pyterrier-caching) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from requests->python-terrier>=0.11.0->pyterrier-caching) (2024.8.30)\n",
      "Requirement already satisfied: heapdict in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from chest->python-terrier>=0.11.0->pyterrier-caching) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from jinja2->python-terrier>=0.11.0->pyterrier-caching) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pandas->python-terrier>=0.11.0->pyterrier-caching) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pandas->python-terrier>=0.11.0->pyterrier-caching) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from pandas->python-terrier>=0.11.0->pyterrier-caching) (2024.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from statsmodels->python-terrier>=0.11.0->pyterrier-caching) (1.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (2.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->python-terrier>=0.11.0->pyterrier-caching) (1.17.0)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier>=0.11.0->pyterrier-caching) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade tira ir-datasets python-terrier \n",
    "!pip3 install --upgrade pyterrier-caching pyterrier_t5\n",
    "# !pip3 install --upgrade git+https://github.com/terrierteam/pyterrier_t5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import get_dataset\n",
    "\n",
    "pt_dataset = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')\n",
    "pt_dataset_new = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training')\n",
    "pt_dataset_test = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indizes erstellen\n",
    "- für alle drei Datensätze wird ein Index erstellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training documents:  36%|███▌      | 24291/68261 [00:02<00:03, 14632.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:05:53.541 [ForkJoinPool-1-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (6114613) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training documents: 100%|██████████| 68261/68261 [00:04<00:00, 14525.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:05:56.529 [ForkJoinPool-1-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n"
     ]
    }
   ],
   "source": [
    "from pyterrier import IterDictIndexer\n",
    "\n",
    "indexer = IterDictIndexer(\n",
    "    # Store the index in the `index` directory.\n",
    "    \"../data/index\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    # If an index already exists there, then overwrite it.\n",
    "    overwrite=True,\n",
    ")\n",
    "index = indexer.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training documents: 100%|██████████| 113227/113227 [00:30<00:00, 3721.05it/s]\n"
     ]
    }
   ],
   "source": [
    "indexer_new = IterDictIndexer(\n",
    "    # Store the index in the `index` directory.\n",
    "    \"../data/index_new\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    # If an index already exists there, then overwrite it.\n",
    "    overwrite=True,\n",
    ")\n",
    "index_new = indexer_new.index(pt_dataset_new.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test documents: 100%|██████████| 125112/125112 [00:27<00:00, 4575.25it/s]\n"
     ]
    }
   ],
   "source": [
    "indexer_test = IterDictIndexer(\n",
    "    # Store the index in the `index` directory.\n",
    "    \"../data/index_test\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    # If an index already exists there, then overwrite it.\n",
    "    overwrite=True,\n",
    ")\n",
    "index_test = indexer_test.index(pt_dataset_test.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines definieren\n",
    "- BM25 mit den jeweiligen Indizies\n",
    "- MonoT5 in gecachter Variante\n",
    "- DuoT5 in Kombination mit MonoT5 (gecacht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")\n",
    "bm25_new = pt.terrier.Retriever(index_new, wmodel=\"BM25\")\n",
    "bm25_test = pt.terrier.Retriever(index_test, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_t5 import MonoT5ReRanker, DuoT5ReRanker\n",
    "from pyterrier_caching import SparseScorerCache\n",
    "_monoT5 = MonoT5ReRanker()\n",
    "duoT5 = DuoT5ReRanker()\n",
    "\n",
    "def mono_factory(cutoff, bm25_base, mono_t5, dataset):\n",
    "    return (bm25_base % cutoff >> pt.text.get_text(dataset, \"text\") >> mono_t5) ^ bm25_base\n",
    "\n",
    "def duo_factory(cutoff, mono_pipeline):\n",
    "    return (mono_pipeline % cutoff >> duoT5) ^ mono_pipeline\n",
    "\n",
    "monoT5 = SparseScorerCache('monoT5.cache', _monoT5, verbose=True)\n",
    "monoT5_new = SparseScorerCache('monoT5_new.cache', _monoT5, verbose=True)\n",
    "monoT5_test = SparseScorerCache('monoT5_test.cache', _monoT5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testen auf dem Standard-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x1470fe9b0>, group='query', key='docno'): 91067 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>P_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25+monoT5</td>\n",
       "      <td>0.727375</td>\n",
       "      <td>0.787629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  ndcg_cut_10      P_10\n",
       "0  BM25+monoT5     0.727375  0.787629"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier import Experiment\n",
    "pipeline_mono_t5 = mono_factory(1000, bm25, monoT5, pt_dataset)\n",
    "pipeline_duo_t5 = duo_factory(5, pipeline_mono_t5)\n",
    "# Run experiment\n",
    "Experiment(\n",
    "    retr_systems=[\n",
    "        pipeline_mono_t5,\n",
    "#        pipeline_duo_t5,\n",
    "    ],\n",
    "    names=[\n",
    "        \"BM25+monoT5\",\n",
    "#        \"BM25+monoT5+duoT5\",\n",
    "    ],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"ndcg_cut_10\", \"P_10\"],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimieren von MonoT5 auf dem Standard-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 2405 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 25 , NDCG@10: 0.622\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 4783 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 50 , NDCG@10: 0.662\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 7158 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 75 , NDCG@10: 0.68\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 9533 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 100 , NDCG@10: 0.684\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 11908 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 125 , NDCG@10: 0.684\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 14283 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 150 , NDCG@10: 0.695\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 16658 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 175 , NDCG@10: 0.704\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 19017 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 200 , NDCG@10: 0.704\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 21367 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 225 , NDCG@10: 0.707\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 23717 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 250 , NDCG@10: 0.706\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 26057 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 275 , NDCG@10: 0.709\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 28382 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 300 , NDCG@10: 0.711\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 30707 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 325 , NDCG@10: 0.719\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 33027 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 350 , NDCG@10: 0.722\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 35327 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 375 , NDCG@10: 0.723\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 37627 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 400 , NDCG@10: 0.726\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 39927 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 425 , NDCG@10: 0.727\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 42227 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 450 , NDCG@10: 0.727\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 44527 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 475 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 46827 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 500 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 525 , NDCG@10: 0.729\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 51410 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 550 , NDCG@10: 0.727\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 53685 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 575 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 55960 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 600 , NDCG@10: 0.727\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 58225 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 625 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 60475 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 650 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 62712 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 675 , NDCG@10: 0.729\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 64937 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 700 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 67162 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 725 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 69387 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 750 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 71601 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 775 , NDCG@10: 0.729\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 73799 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 800 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 75974 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 825 , NDCG@10: 0.729\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 78149 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 850 , NDCG@10: 0.727\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 80324 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 875 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 82499 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 900 , NDCG@10: 0.729\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 84654 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 925 , NDCG@10: 0.729\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 86804 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 950 , NDCG@10: 0.728\n",
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x324f68190>, group='query', key='docno'): 88942 hit(s), 0 miss(es)\n",
      "Mono Cutoff: 975 , NDCG@10: 0.727\n",
      "Der optimale Mono-Cutoff ist: 525\n"
     ]
    }
   ],
   "source": [
    "def optimize_monot5():\n",
    "    max_ndcg = 0\n",
    "    best_cutoff = 0\n",
    "    for mono_cutoff in range(25, 1000, 25):\n",
    "        pipeline_mono_t5 = mono_factory(mono_cutoff, bm25, monoT5, pt_dataset)\n",
    "        exp = Experiment(\n",
    "            [pipeline_mono_t5],\n",
    "            topics = pt_dataset.get_topics('text'),\n",
    "            qrels = pt_dataset.get_qrels(),\n",
    "            eval_metrics =['ndcg_cut_10'],\n",
    "            names=[\"monoT5\"],\n",
    "            round = 3,\n",
    "            baseline=0\n",
    "        )\n",
    "        new_ndcg = exp['ndcg_cut_10'][0]\n",
    "        if exp['ndcg_cut_10'][0] > max_ndcg:\n",
    "            max_ndcg = new_ndcg\n",
    "            best_cutoff = mono_cutoff\n",
    "        print('Mono Cutoff:', mono_cutoff, ', NDCG@10:', exp['ndcg_cut_10'][0])\n",
    "    return best_cutoff\n",
    "\n",
    "mono_cutoff = optimize_monot5()\n",
    "print(\"Der optimale Mono-Cutoff ist:\", mono_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimierung von DuoT5 auf dem Standard-Datensatz\n",
    "- es wird der beste ermittelte MonoT5-Cutoff (525) genutzt und damit verschiedene DuoT5-Cutoffs (3,5,10,15) getestet\n",
    "- der beste duoT5-Cutoff wird gewählt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duoT5: 100%|██████████| 97/97 [01:12<00:00,  1.34queries/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "Duo Cutoff: 3 , NDCG@10: 0.73\n",
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duoT5: 100%|██████████| 97/97 [04:23<00:00,  2.72s/queries]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "Duo Cutoff: 5 , NDCG@10: 0.733\n",
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duoT5: 100%|██████████| 97/97 [27:07<00:00, 16.77s/queries]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "Duo Cutoff: 10 , NDCG@10: 0.735\n",
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duoT5: 100%|██████████| 97/97 [56:19<00:00, 34.84s/queries]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_fix.cache', <pyterrier_t5.MonoT5ReRanker object at 0x15047e7d0>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "Duo Cutoff: 15 , NDCG@10: 0.754\n",
      "Der optimale Duo-Cutoff ist: 15\n"
     ]
    }
   ],
   "source": [
    "def optimize_duot5(mono_cutoff = 525):\n",
    "    max_ndcg = 0\n",
    "    best_cutoff = 0\n",
    "    pipeline_mono_t5 = mono_factory(mono_cutoff, bm25, monoT5, pt_dataset)\n",
    "    for duo_cutoff in [3,5,10,15]:\n",
    "        pipeline_duo_t5 = duo_factory(duo_cutoff, pipeline_mono_t5)\n",
    "        exp = Experiment(\n",
    "            [pipeline_duo_t5],\n",
    "            topics = pt_dataset.get_topics('text'),\n",
    "            qrels = pt_dataset.get_qrels(),\n",
    "            eval_metrics =['ndcg_cut_10'],\n",
    "            names=[\"duoT5\"],\n",
    "            round = 3,\n",
    "            baseline=0\n",
    "        )\n",
    "        new_ndcg = exp['ndcg_cut_10'][0]\n",
    "        if exp['ndcg_cut_10'][0] > max_ndcg:\n",
    "            max_ndcg = new_ndcg\n",
    "            best_cutoff = duo_cutoff\n",
    "        print('Duo Cutoff:', duo_cutoff, ', NDCG@10:', exp['ndcg_cut_10'][0])\n",
    "    return best_cutoff\n",
    "\n",
    "duo_cutoff = optimize_duot5(mono_cutoff)\n",
    "print(\"Der optimale Duo-Cutoff ist:\", duo_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_cutoff = 525\n",
    "duo_cutoff = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abgabe der Runs\n",
    "- je Datensatz zwei Runs (MonoT5 und MonoT5+DuoT5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run für Standard-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x3581e5720>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../data/run_bl_standard\".\n",
      "Done. run file is stored under \"../data/run_bl_standard/run.txt.gz\".\n",
      "Run uploaded to TIRA. Claim ownership via: https://www.tira.io/claim-submission/fedf35dc-f3b2-4d1b-b1ad-b4e617eb437f\n"
     ]
    }
   ],
   "source": [
    "from tira.third_party_integrations import persist_and_normalize_run\n",
    "\n",
    "pipeline = mono_factory(mono_cutoff, bm25, monoT5, pt_dataset)\n",
    "run = pipeline(pt_dataset.get_topics('text'))\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='monoT5-BL-suchMaschinen', \n",
    "    default_output='../data/run_bl_standard',\n",
    "    upload_to_tira=pt_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x3581e5720>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duoT5: 100%|██████████| 97/97 [3:06:10<00:00, 115.16s/queries]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5.cache', <pyterrier_t5.MonoT5ReRanker object at 0x3581e5720>, group='query', key='docno'): 49127 hit(s), 0 miss(es)\n",
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../data/run_duo_standard\".\n",
      "Done. run file is stored under \"../data/run_duo_standard/run.txt.gz\".\n",
      "Run uploaded to TIRA. Claim ownership via: https://www.tira.io/claim-submission/553c3bf5-99fc-429d-b6b3-7426fc34b965\n"
     ]
    }
   ],
   "source": [
    "duo_pipeline = duo_factory(duo_cutoff, pipeline)\n",
    "run = duo_pipeline(pt_dataset.get_topics('text'))\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='duoT5-suchMaschinen', \n",
    "    default_output='../data/run_duo_standard',\n",
    "    upload_to_tira=pt_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run für Traingsdatensatz Neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|██████████| 11157/11157 [9:57:34<00:00,  3.21s/batches]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_new.cache', <pyterrier_t5.MonoT5ReRanker object at 0x3581e5720>, group='query', key='docno'): 525 hit(s), 44625 miss(es)\n",
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../data/run_bl_new\".\n",
      "Done. run file is stored under \"../data/run_bl_new/run.txt.gz\".\n",
      "Run uploaded to TIRA. Claim ownership via: https://www.tira.io/claim-submission/5c88070a-6bf1-4a7e-be11-4b021972af04\n"
     ]
    }
   ],
   "source": [
    "pipeline = mono_factory(mono_cutoff, bm25_new, monoT5_new, pt_dataset_new)\n",
    "run = pipeline(pt_dataset_new.get_topics('text'))\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='monoT5-BL-suchMaschinen', \n",
    "    default_output='../data/run_bl_new',\n",
    "    upload_to_tira=pt_dataset_new,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite3ScorerCache('monoT5_new.cache', <pyterrier_t5.MonoT5ReRanker object at 0x3581e5720>, group='query', key='docno'): 45150 hit(s), 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duoT5:   2%|▏         | 2/86 [03:10<2:13:24, 95.29s/queries]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m duo_pipeline \u001b[38;5;241m=\u001b[39m duo_factory(duo_cutoff, pipeline)\n\u001b[0;32m----> 2\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mduo_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_dataset_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m persist_and_normalize_run(\n\u001b[1;32m      4\u001b[0m     run,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Give your approach a short but descriptive name tag.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     upload_to_tira\u001b[38;5;241m=\u001b[39mpt_dataset_new,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/pyterrier/transformer.py:161\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Runs the transformer for the given input and returns its output as the same type as the input.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    :rtype: ``pd.DataFrame``, ``Iterable[Dict]``, ``List[Dict]``\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_iter(inp)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/pyterrier/_ops.py:137\u001b[0m, in \u001b[0;36mConcatenate.transform\u001b[0;34m(self, topics_and_res)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# take the first set as the top of the ranking\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m res1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics_and_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# identify the lowest score for each query\u001b[39;00m\n\u001b[1;32m    139\u001b[0m last_scores \u001b[38;5;241m=\u001b[39m res1[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lastscore\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/pyterrier/_ops.py:372\u001b[0m, in \u001b[0;36mCompose.transform\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    370\u001b[0m out \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformers:\n\u001b[0;32m--> 372\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/pyterrier_t5/__init__.py:123\u001b[0m, in \u001b[0;36mDuoT5ReRanker.transform\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m    121\u001b[0m enc \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m enc\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    124\u001b[0m result \u001b[38;5;241m=\u001b[39m result[:, \u001b[38;5;241m0\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNREL)]\n\u001b[1;32m    125\u001b[0m result \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(result, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1854\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1864\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1865\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1866\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1867\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1868\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:675\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[0;32m--> 675\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    686\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:593\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[1;32m    592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:552\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    549\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype_as(scores)\n\u001b[1;32m    553\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "duo_pipeline = duo_factory(duo_cutoff, pipeline)\n",
    "run = duo_pipeline(pt_dataset_new.get_topics('text'))\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='duoT5-suchMaschinen', \n",
    "    default_output='../data/run_duo_new',\n",
    "    upload_to_tira=pt_dataset_new,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run für Testdatensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = mono_factory(mono_cutoff, bm25_test, monoT5_test, pt_dataset_test)\n",
    "run = pipeline(pt_dataset_test.get_topics('text'))\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='monoT5-BL-suchMaschinen', \n",
    "    default_output='../data/run_bl_test',\n",
    "    upload_to_tira=pt_dataset_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duo_pipeline = duo_factory(duo_cutoff, pipeline)\n",
    "run = duo_pipeline(pt_dataset_test.get_topics('text'))\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='duoT5-suchMaschinen', \n",
    "    default_output='../data/run_duo_test',\n",
    "    upload_to_tira=pt_dataset_test,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
