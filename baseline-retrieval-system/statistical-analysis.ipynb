{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab Tutorial: Statistical Analysis\n",
    "\n",
    "This tutorial shows how to conduct a hypothesis test to compare two retrieval approaches.\n",
    "The two runs compared in this example are loaded from the TIRA cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Ensure that libraries are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n",
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.7 (build: craigm 2022-11-10 18:30), helper_version=0.0.7]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/tira/third_party_integrations.py:39: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "The following code will have the same effect:\n",
      "pt.java.add_package('com.github.terrierteam', 'terrier-prf', '-SNAPSHOT')\n",
      "pt.terrier.set_version('5.7')\n",
      "pt.terrier.set_helper_version('0.0.7')\n",
      "pt.java.mavenresolver.offline()\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init(\n"
     ]
    }
   ],
   "source": [
    "# This command loads and starts PyTerrier so that it also works in TIRA.\n",
    "\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "\n",
    "ensure_pyterrier_is_loaded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2z/hftmcxnx6857z5y2jcyv934h0000gn/T/ipykernel_21858/811612812.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not started():\n"
     ]
    }
   ],
   "source": [
    "# PyTerrier must be imported after `ensure_pyterrier_is_loaded` is called.\n",
    "\n",
    "from pyterrier import started, init\n",
    "\n",
    "if not started():\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRDSDataset('ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier import get_dataset\n",
    "\n",
    "dataset_train = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRDSDataset('ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_validation = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training')\n",
    "dataset_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRDSDataset('ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development, let's use the training set for the experiments.\n",
    "dataset = dataset_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the retrieval pipeline with TIRA\n",
    "\n",
    "In this example, we will use two existing submitted runs and load the approaches via the TIRA API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.rest_api_client import Client\n",
    "\n",
    "tira_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach IDs below follow the structure: `<task>/<team>/<submission>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 1.97MiB [00:00, 5.25MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /Users/till/.tira/extracted_runs/ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training/ir-wise-24-suchmaschinen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tira.pyterrier_util.TiraSourceTransformer at 0x13fc03f70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approach_baseline = tira_client.pt.from_retriever_submission(\n",
    "    approach='ir-lab-wise-2024/ir-wise-24-suchmaschinen/BM25 + ReRanking (monoT5 BL)',\n",
    "    dataset='subsampled-ms-marco-rag-20250105-training',\n",
    ")\n",
    "approach_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach_baseline = tira_client.pt.from_retriever_submission(\n",
    "#     approach='ir-lab-wise-2024/ir-wise-24-tutors/Retrieval Baseline',\n",
    "#     dataset='subsampled-ms-marco-deep-learning-20241201-training',\n",
    "# )\n",
    "# approach_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tira.pyterrier_util.TiraSourceTransformer at 0x12fc102b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approach_new = tira_client.pt.from_retriever_submission(\n",
    "    approach='ir-lab-wise-2024/ir-wise-24-suchmaschinen/BM25 + ReRanking (mono+duoT5)',\n",
    "    dataset='subsampled-ms-marco-rag-20250105-training',\n",
    ")\n",
    "approach_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 1.37MiB [00:00, 15.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /Users/till/.tira/extracted_runs/ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training/ir-wise-24-th25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tira.pyterrier_util.TiraSourceTransformer at 0x1231b7b80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# approach_new = tira_client.pt.from_retriever_submission(\n",
    "#     approach='ir-lab-wise-2024/ir-wise-24-th25/BM25 + MonoT5 Rerank',\n",
    "#     dataset='subsampled-ms-marco-deep-learning-20241201-training',\n",
    "# )\n",
    "# approach_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Measure effectiveness\n",
    "\n",
    "Now let us measure the nDCG@10 effectiveness of both systems on the TouchÃ© 2020 task 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/Users/till/.tira/extracted_datasets/ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training/truth-data/qrels.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyterrier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Experiment\n\u001b[1;32m      3\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(\n\u001b[1;32m      4\u001b[0m     retr_systems\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      5\u001b[0m         approach_baseline,\n\u001b[1;32m      6\u001b[0m         approach_new,\n\u001b[1;32m      7\u001b[0m     ],\n\u001b[1;32m      8\u001b[0m     topics\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mget_topics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m----> 9\u001b[0m     qrels\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qrels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     10\u001b[0m     eval_metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg_cut_10\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     names\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonoT5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonoT5+duoT5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     ],\n\u001b[1;32m     15\u001b[0m     perquery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m experiment\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/pyterrier/datasets.py:500\u001b[0m, in \u001b[0;36mIRDSDataset.get_qrels\u001b[0;34m(self, variant)\u001b[0m\n\u001b[1;32m    498\u001b[0m qrel_fields \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m qrelcls\u001b[38;5;241m.\u001b[39m_fields \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m variant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m variant \u001b[38;5;129;01min\u001b[39;00m qrel_fields, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_irds_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m only supports the following qrel variants \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqrel_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 500\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqrels_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# pyterrier uses \"qid\" and \"docno\"\u001b[39;00m\n\u001b[1;32m    503\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocno\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/pandas/core/frame.py:843\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    841\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/ir_datasets/formats/trec.py:433\u001b[0m, in \u001b[0;36mTrecQrels.qrels_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qrels_internal_iter(dlc)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qrels_internal_iter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qrels_dlc)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/ir_datasets/formats/trec.py:436\u001b[0m, in \u001b[0;36mTrecQrels._qrels_internal_iter\u001b[0;34m(self, dlc)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_qrels_internal_iter\u001b[39m(\u001b[38;5;28mself\u001b[39m, dlc):\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dlc\u001b[38;5;241m.\u001b[39mstream() \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    437\u001b[0m         f \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetreader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)(f)\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/ir_datasets/util/download.py:208\u001b[0m, in \u001b[0;36mLocalDownload.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ir/lib/python3.10/site-packages/ir_datasets/util/download.py:203\u001b[0m, in \u001b[0;36mLocalDownload.path\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message:\n\u001b[1;32m    202\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message)\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /Users/till/.tira/extracted_datasets/ir-lab-wise-2024/subsampled-ms-marco-rag-20250105-training/truth-data/qrels.txt"
     ]
    }
   ],
   "source": [
    "from pyterrier.pipelines import Experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    retr_systems=[\n",
    "        approach_baseline,\n",
    "        approach_new,\n",
    "    ],\n",
    "    topics=dataset.get_topics(\"query\"),\n",
    "    qrels=dataset.get_qrels(),\n",
    "    eval_metrics=[\"ndcg_cut_10\"],\n",
    "    names=[\n",
    "        \"monoT5\",\n",
    "        \"monoT5+duoT5\",\n",
    "    ],\n",
    "    perquery=True,\n",
    ")\n",
    "experiment.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame shows the nDCG@10 values measured for each query and both systems. \\\n",
    "So we have pairs of measurements where the same metric (i.e., nDCG@10) is measured using the same input (e.g., query #1) but for two different systems.\n",
    "Let's re-arrange the data frame so that the effectiveness values are in separate columns, not rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>measure</th>\n",
       "      <th>value_baseline</th>\n",
       "      <th>value_approach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1030303</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.627356</td>\n",
       "      <td>0.733750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1037496</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.912539</td>\n",
       "      <td>0.902150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1037798</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.199613</td>\n",
       "      <td>0.220807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1043135</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.845994</td>\n",
       "      <td>0.730450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104861</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1051399</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.821843</td>\n",
       "      <td>0.817671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1063750</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.871021</td>\n",
       "      <td>0.841178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1064670</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.801862</td>\n",
       "      <td>0.752252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1071750</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.726809</td>\n",
       "      <td>0.688409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1103812</td>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.744751</td>\n",
       "      <td>0.659619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid      measure  value_baseline  value_approach\n",
       "0  1030303  ndcg_cut_10        0.627356        0.733750\n",
       "1  1037496  ndcg_cut_10        0.912539        0.902150\n",
       "2  1037798  ndcg_cut_10        0.199613        0.220807\n",
       "3  1043135  ndcg_cut_10        0.845994        0.730450\n",
       "4   104861  ndcg_cut_10        1.000000        1.000000\n",
       "5  1051399  ndcg_cut_10        0.821843        0.817671\n",
       "6  1063750  ndcg_cut_10        0.871021        0.841178\n",
       "7  1064670  ndcg_cut_10        0.801862        0.752252\n",
       "8  1071750  ndcg_cut_10        0.726809        0.688409\n",
       "9  1103812  ndcg_cut_10        0.744751        0.659619"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_baseline = experiment[experiment[\"name\"] == \"monoT5\"]\\\n",
    "    .drop(columns=[\"name\"])\n",
    "experiment_approach = experiment[experiment[\"name\"] == \"monoT5+duoT5\"]\\\n",
    "    .drop(columns=[\"name\"])\n",
    "\n",
    "experiment_paired = experiment_baseline.merge(\n",
    "    experiment_approach,\n",
    "    on=[\"qid\", \"measure\"],\n",
    "    suffixes=(\"_baseline\", \"_approach\"),\n",
    ")\n",
    "experiment_paired.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7285645438929027, 0.7296680238639147)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Durchschnitt berechnen\n",
    "import numpy as np\n",
    "sum_bl = sum(experiment_paired[\"value_baseline\"]) / len(experiment_paired[\"value_baseline\"])\n",
    "sum_new = sum(experiment_paired[\"value_approach\"]) / len(experiment_paired[\"value_approach\"])\n",
    "sum_bl, sum_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Conduct hypothesis tests\n",
    "\n",
    "On this _paired_ measurement data, we can now conduct _paired_ t-tests to test for statistical significance of given hypotheses.\n",
    "Remember that the choice of your test depends (amongst other factors) on how the hypothesis is formulated.\n",
    "\n",
    "Let us test some hypotheses to get a feeling of what this means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis 1: The new approach has a significantly different nDCG@10 on the chosen dataset than the baseline.\n",
    "(Hint: For your own tests, you'd want to replace the approach and dataset names with the actual names above.)\n",
    "\n",
    "Significance test: two-sided paired t-test \\\n",
    "Significance level: $\\alpha = 0.05$ (i.e., the effect is only considered significant if $p < 0.05$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316784914254621"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "ttest_rel(\n",
    "    experiment_paired[\"value_approach\"],\n",
    "    experiment_paired[\"value_baseline\"],\n",
    "    alternative='two-sided',\n",
    ").pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above value is called $p$, the probability of the corresponding null hypothesis (the probability that the effect would be observed by chance). \\\n",
    "If this is lower than our significance level $\\alpha$, we can reject the null hypothesis and confirm the hypothesis 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it would be great to find out which is better. \\\n",
    "One way could be to formulate a hypothesis with a predefined \"direction\". In this example we assume our new approach to be better.\n",
    "\n",
    "#### Hypothesis 2: The new approach has a significantly higher nDCG@10 on the chosen dataset than the baseline.\n",
    "\n",
    "Significance test: one-sided paired t-test \\\n",
    "Significance level: $\\alpha = 0.05$ (or $p < 0.05$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41583924571273106"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "ttest_rel(\n",
    "    experiment_paired[\"value_approach\"],\n",
    "    experiment_paired[\"value_baseline\"],\n",
    "    alternative='greater',\n",
    ").pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if the probability $p$ of the null hypothesis is lower than our significance level $\\alpha$, then we can reject the null hypothesis and confirm hypothesis 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the opposite direction: the new approach could be worse w.r.t. nDCG@10 than the baseline.\n",
    "\n",
    "#### Hypothesis 2: The new approach has a significantly lower nDCG@10 on the chosen dataset than the baseline.\n",
    "\n",
    "Significance test: one-sided paired t-test \\\n",
    "Significance level: $\\alpha = 0.05$ (or $p < 0.05$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5841607542872689"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "ttest_rel(\n",
    "    experiment_paired[\"value_approach\"],\n",
    "    experiment_paired[\"value_baseline\"],\n",
    "    alternative='less',\n",
    ").pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if the probability $p$ of the null hypothesis is lower than our significance level $\\alpha$, then we can reject the null hypothesis and confirm hypothesis 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothese: Die Werte sind gleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017256667464262042 0.04448322255769013\n",
      "p = 0.04448322255769013\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "epsilon = 0.01\n",
    "p_greater = ttest_rel(\n",
    "    experiment_paired[\"value_approach\"],\n",
    "    experiment_paired[\"value_baseline\"] - epsilon,\n",
    "    alternative='greater',\n",
    ").pvalue\n",
    "p_less = ttest_rel(\n",
    "    experiment_paired[\"value_approach\"],\n",
    "    experiment_paired[\"value_baseline\"] + epsilon,\n",
    "    alternative='less',\n",
    ").pvalue\n",
    "\n",
    "print(\"p =\", max(p_greater, p_less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
